# Fine-Tuning Concepts

## Overview

Fine-tuning trains a pre-trained LLM on your own dataset to adapt it for a specific task, domain, or style. The result is a model that performs your specific task better, faster, and cheaper than a general-purpose model with a long system prompt.

But fine-tuning is often the wrong tool. Before reaching for it, you should exhaust prompt engineering and RAG — both are faster, cheaper, and easier to update. Fine-tuning makes sense when you have a well-defined task, hundreds or thousands of examples, and clear evidence that prompting alone isn't good enough.

## Prerequisites

- LLM Fundamentals (tokens, context window, temperature)
- Prompt Engineering basics
- Python (most fine-tuning tooling is Python-first)
- Access to training infrastructure (GPU or cloud API)

## Core Concepts

### When to Fine-Tune vs When to Prompt

| Scenario | Approach |
|----------|----------|
| General task with a clear prompt | Prompt engineering |
| Domain-specific knowledge | RAG (retrieval-augmented generation) |
| Specific output format/style | Prompt engineering with examples (few-shot) |
| Task too complex for prompting alone | Fine-tuning |
| Need to reduce prompt size for cost | Fine-tuning |
| Proprietary data that shouldn't be in prompts | Fine-tuning |
| Need consistent tone/style at scale | Fine-tuning |
| Latest information (post training cutoff) | RAG, not fine-tuning |

**Decision tree:**
1. Can a strong system prompt + few-shot examples solve it? → Use prompting
2. Does the task require domain knowledge? → Try RAG first
3. Is prompting too slow/expensive at your volume? → Consider fine-tuning
4. Do you have 500+ high-quality examples? → Fine-tuning may be viable
5. Does the task change frequently? → Stick to prompting (fine-tuning doesn't update easily)

### Full Fine-Tuning vs Parameter-Efficient Fine-Tuning

**Full fine-tuning** — updates all model weights. Expensive (requires dozens of GPUs for large models), risk of catastrophic forgetting. Rarely needed for application use cases.

**Parameter-efficient fine-tuning (PEFT)** — freezes most weights and only trains a small set of additional parameters. Dramatically reduces memory and compute requirements.

**LoRA (Low-Rank Adaptation)** — the most popular PEFT method. Instead of updating the weight matrix W directly, LoRA adds two small matrices A and B such that the update is W + BA (where B is tall and thin, A is short and wide). The rank `r` controls how many parameters are trained.

```
Full model: 7B weights (14GB in fp16)
LoRA adapter: 10-50MB of trained weights
At inference: W_effective = W_original + B * A
```

**QLoRA (Quantized LoRA)** — combines 4-bit quantization of the frozen weights with LoRA adapters. Allows fine-tuning 7B models on a single 24GB GPU.

```python
# QLoRA setup with Hugging Face + bitsandbytes
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    quantization_config=bnb_config,
    device_map="auto",
)

model = prepare_model_for_kbit_training(model)

peft_config = LoraConfig(
    r=16,                       # rank: higher = more parameters, more capability
    lora_alpha=32,              # scaling factor; usually 2x rank
    target_modules=["q_proj", "v_proj"],  # which attention matrices to train
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
# trainable params: 6,815,744 || all params: 8,036,421,632 || trainable%: 0.0848
```

### Dataset Preparation

The quality of your training data is the single most important factor. 500 high-quality examples beat 5,000 noisy ones.

**Dataset format** (instruction tuning):
```json
[
  {
    "messages": [
      {
        "role": "system",
        "content": "You are a customer support agent for Acme Corp. Be concise and helpful."
      },
      {
        "role": "user",
        "content": "How do I reset my password?"
      },
      {
        "role": "assistant",
        "content": "To reset your password:\n1. Go to the login page\n2. Click 'Forgot password'\n3. Enter your email\n4. Check your inbox for the reset link\n\nThe link expires in 24 hours. Let me know if you need help!"
      }
    ]
  }
]
```

**Data quality checklist:**
- [ ] Consistent format (same system prompt structure throughout)
- [ ] Correct outputs (reviewed by domain experts, not just generated by another model)
- [ ] Diverse coverage (different phrasings, edge cases, difficulty levels)
- [ ] No duplicates (deduplication with fuzzy matching)
- [ ] No PII in training data (GDPR/LGPD compliance)
- [ ] Balanced distribution (equal representation of all classes/cases)

**Minimum dataset sizes:**
| Task | Minimum | Good |
|------|---------|------|
| Style/format adaptation | 100-200 | 500+ |
| Domain-specific Q&A | 500 | 2,000+ |
| Classification | 200-500 per class | 1,000+ per class |
| Code generation | 1,000 | 5,000+ |

### OpenAI Fine-Tuning API

The simplest path to fine-tuning — no GPU management required.

```python
# Python (fine-tuning is primarily a Python workflow)
from openai import OpenAI
import json

client = OpenAI(api_key="...")

# 1. Prepare training data
training_data = [
    {
        "messages": [
            {"role": "system", "content": "Classify the sentiment of the review."},
            {"role": "user", "content": "Great product, works perfectly!"},
            {"role": "assistant", "content": "POSITIVE"}
        ]
    },
    # ... hundreds more examples
]

# Save as JSONL
with open("training.jsonl", "w") as f:
    for example in training_data:
        f.write(json.dumps(example) + "\n")

# 2. Upload training file
with open("training.jsonl", "rb") as f:
    file_response = client.files.create(file=f, purpose="fine-tune")

training_file_id = file_response.id
print(f"Uploaded: {training_file_id}")

# 3. Create fine-tuning job
job = client.fine_tuning.jobs.create(
    training_file=training_file_id,
    model="gpt-4o-mini-2024-07-18",  # cheaper base model
    hyperparameters={
        "n_epochs": 3,          # number of training passes
        "batch_size": 4,
        "learning_rate_multiplier": 1.8,
    },
    suffix="sentiment-v1",      # added to model name
)

print(f"Job ID: {job.id}")
```

```typescript
// Monitoring the fine-tuning job (TypeScript)
import OpenAI from 'openai';
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

async function pollJob(jobId: string) {
  while (true) {
    const job = await openai.fineTuning.jobs.retrieve(jobId);
    console.log(`Status: ${job.status} | Error: ${job.error?.message ?? 'none'}`);

    if (job.status === 'succeeded') {
      console.log(`Fine-tuned model: ${job.fine_tuned_model}`);
      return job.fine_tuned_model;
    }

    if (job.status === 'failed' || job.status === 'cancelled') {
      throw new Error(`Job ${job.status}: ${job.error?.message}`);
    }

    await new Promise((r) => setTimeout(r, 30_000)); // check every 30s
  }
}

// Using the fine-tuned model
const fineTunedModel = await pollJob('ftjob-abc123');
const response = await openai.chat.completions.create({
  model: fineTunedModel,
  messages: [
    { role: 'system', content: 'Classify the sentiment of the review.' },
    { role: 'user', content: 'Average product, nothing special.' },
  ],
  max_tokens: 10,
  temperature: 0,
});
```

### Evaluation Metrics

**For classification tasks:**
- **Accuracy** — % of correct predictions
- **Precision/Recall/F1** — for imbalanced classes
- **Confusion matrix** — see where the model makes mistakes

**For generation tasks:**
- **BLEU score** — n-gram overlap with reference (good for translation, poor for open-ended)
- **ROUGE** — recall-based overlap (good for summarization)
- **BERTScore** — semantic similarity using embeddings
- **Human evaluation** — expensive but most reliable
- **LLM-as-judge** — use a stronger model to rate outputs on a rubric

```python
# Python evaluation script
from sklearn.metrics import accuracy_score, classification_report
import json

def evaluate_model(model_name: str, test_data: list[dict]) -> dict:
    predictions = []
    true_labels = []

    for example in test_data:
        user_message = example["messages"][-2]["content"]
        true_label = example["messages"][-1]["content"].strip()

        response = client.chat.completions.create(
            model=model_name,
            messages=example["messages"][:-1],  # exclude assistant turn
            max_tokens=20,
            temperature=0,
        )
        prediction = response.choices[0].message.content.strip()

        predictions.append(prediction)
        true_labels.append(true_label)

    accuracy = accuracy_score(true_labels, predictions)
    report = classification_report(true_labels, predictions)

    return {
        "accuracy": accuracy,
        "report": report,
        "n_examples": len(test_data),
    }

# Compare base vs fine-tuned
base_metrics = evaluate_model("gpt-4o-mini", test_data)
finetuned_metrics = evaluate_model("ft:gpt-4o-mini-2024-07-18:acme::abc123", test_data)

print(f"Base model accuracy: {base_metrics['accuracy']:.3f}")
print(f"Fine-tuned accuracy: {finetuned_metrics['accuracy']:.3f}")
print(f"Improvement: +{(finetuned_metrics['accuracy'] - base_metrics['accuracy']):.3f}")
```

## Hands-On Examples

### TypeScript Pipeline: Training Data Generation

When you don't have thousands of labeled examples, you can generate synthetic data from a small seed set:

```typescript
import Anthropic from '@anthropic-ai/sdk';
import { writeFileSync } from 'fs';

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });

interface TrainingExample {
  messages: Array<{ role: string; content: string }>;
}

const SEED_EXAMPLES = [
  {
    input: 'My order hasn\'t arrived after 2 weeks',
    output: JSON.stringify({
      category: 'SHIPPING',
      urgency: 'HIGH',
      action: 'Check carrier tracking and initiate investigation if package is lost',
    }),
  },
  // ... 10-20 seed examples
];

async function generateSyntheticData(
  count: number,
  systemPrompt: string
): Promise<TrainingExample[]> {
  const examples: TrainingExample[] = [];
  const seedContext = SEED_EXAMPLES.map(
    (e) => `Input: ${e.input}\nOutput: ${e.output}`
  ).join('\n\n');

  for (let i = 0; i < count; i++) {
    const response = await anthropic.messages.create({
      model: 'claude-sonnet-4-6',
      max_tokens: 500,
      temperature: 0.8,  // high temp for variety
      messages: [{
        role: 'user',
        content: `Generate a new realistic customer support message and its correct classification.
The message should be different from these examples but follow the same pattern:

${seedContext}

Return JSON: {"input": "customer message", "output": "classification JSON"}`,
      }],
    });

    try {
      const { input, output } = JSON.parse(
        (response.content[0] as Anthropic.TextBlock).text
      );

      examples.push({
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: input },
          { role: 'assistant', content: output },
        ],
      });
    } catch {
      // Skip malformed examples
    }

    if (i % 50 === 0) console.log(`Generated ${i}/${count} examples`);
  }

  return examples;
}

async function main() {
  const systemPrompt = 'Classify customer support tickets. Return JSON with category, urgency, and action.';
  const syntheticData = await generateSyntheticData(500, systemPrompt);

  // Split 80/20 train/validation
  const splitAt = Math.floor(syntheticData.length * 0.8);
  const trainData = syntheticData.slice(0, splitAt);
  const validData = syntheticData.slice(splitAt);

  // Write as JSONL
  writeFileSync('train.jsonl', trainData.map((e) => JSON.stringify(e)).join('\n'));
  writeFileSync('valid.jsonl', validData.map((e) => JSON.stringify(e)).join('\n'));
  console.log(`Wrote ${trainData.length} train, ${validData.length} valid examples`);
}

main();
```

### Hyperparameter Guidance

```python
# For most classification and extraction tasks:
hyperparameters = {
    "n_epochs": 3,              # 1-5; more epochs = more overfitting risk
    "batch_size": 4,            # 4-32; larger batches = more stable gradients
    "learning_rate_multiplier": 1.8,  # 0.1-2.0; higher = faster but unstable
}

# For style/format adaptation:
hyperparameters = {
    "n_epochs": 2,              # less epochs to avoid forgetting base capabilities
    "batch_size": 8,
    "learning_rate_multiplier": 1.0,
}

# For small datasets (<200 examples):
hyperparameters = {
    "n_epochs": 5,              # more epochs compensates for less data
    "batch_size": 2,
    "learning_rate_multiplier": 0.5,  # lower LR for small data
}
```

## Common Patterns & Best Practices

### The Fine-Tuning Iteration Cycle

```
1. Start with prompting — establish a baseline
2. Collect failures — cases where prompting fails
3. Build eval set — 100+ examples with correct labels
4. Fine-tune on failures — not all data, just hard cases
5. Evaluate — measure improvement on eval set
6. Ship to shadow traffic — compare against prompt baseline
7. Iterate — add more data for remaining failure modes
```

### Data Formatting Best Practices

```python
# 1. Consistent system prompt
SYSTEM_PROMPT = "You are a customer support classifier. Return a JSON object."
# Use the EXACT same system prompt in all examples and at inference time

# 2. Consistent output format
# Bad: some examples return {"category":"X"}, others return "Category: X"
# Good: always return the same format

# 3. Include negative examples
negative_examples = [
    {
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": "Hello, how are you?"},
            {"role": "assistant", "content": '{"category": "OTHER", "urgency": "LOW"}'},
        ]
    }
]
# The model needs to know what to do with out-of-distribution inputs
```

### Preventing Overfitting

```python
# Signs of overfitting:
# - Training loss decreasing, validation loss increasing
# - Model memorizes training examples instead of generalizing
# - Performance on unseen data is poor

# Mitigations:
# 1. More diverse training data
# 2. Fewer epochs (2-3 instead of 5+)
# 3. Lower learning rate
# 4. Add data augmentation (paraphrase examples)
# 5. Use a larger base model (more parameters = less overfitting per example)
```

## Anti-Patterns to Avoid

**Fine-tuning to inject knowledge**
```
# Bad: trying to teach the model facts
"Q: What is Acme Corp's refund policy? A: 30-day no-questions-asked returns."

# Fine-tuning teaches behavior, not facts
# For knowledge: use RAG — retrieve from a document store at inference time
# Facts embedded in fine-tuning become stale and are hard to update
```

**Using low-quality generated data without review**
```python
# Bad: generate 10,000 examples with GPT-4, fine-tune without review
# Result: model learns GPT-4's errors and biases

# Good: generate examples, then have humans review and correct a sample
# Random sample review: check 10% of generated data
# If >5% error rate, regenerate with better prompts or collect real data
```

**Skipping the evaluation baseline**
Always measure base model performance before fine-tuning. If the base model already achieves 90% accuracy, you need fine-tuning for a different reason (cost, latency) not quality.

**Fine-tuning on the entire training dataset without validation split**
Always hold out 15-20% for validation to detect overfitting during training.

**Not versioning training data and models**
```
# Good practice:
dataset_v1.jsonl         → ftjob-abc123 → ft:gpt-4o-mini-...:v1
dataset_v2.jsonl         → ftjob-def456 → ft:gpt-4o-mini-...:v2
# Keep all versions — you may need to roll back
```

## Debugging & Troubleshooting

### Poor Fine-Tuned Performance

```python
# Diagnosis checklist:
# 1. Check training data quality
#    - Sample 20 examples and review manually
#    - Are the outputs correct?
#    - Is the format consistent?

# 2. Check data volume
#    - Too few examples? → collect more or use synthetic augmentation
#    - Too many noisy examples? → filter by quality

# 3. Check the test setup
#    - Are you using the same system prompt as in training?
#    - Is temperature set to 0 for deterministic evaluation?

# 4. Check for train/test distribution mismatch
#    - Is the eval data representative of production data?
#    - Do eval examples look like training examples?
```

### Checking for Data Leakage

```python
from difflib import SequenceMatcher

def check_leakage(train_data, test_data, threshold=0.85):
    leaks = []
    for i, test_ex in enumerate(test_data):
        test_input = test_ex["messages"][1]["content"]
        for j, train_ex in enumerate(train_data):
            train_input = train_ex["messages"][1]["content"]
            ratio = SequenceMatcher(None, test_input, train_input).ratio()
            if ratio > threshold:
                leaks.append((i, j, ratio))

    if leaks:
        print(f"WARNING: {len(leaks)} potential data leaks found")
        for test_i, train_j, sim in leaks[:5]:
            print(f"  Test[{test_i}] ≈ Train[{train_j}] (similarity: {sim:.3f})")
    else:
        print("No significant data leakage detected")
```

## Real-World Scenarios

### Scenario 1: Fine-Tuning for Customer Support Triage

A support team receives 10,000 tickets per month. Manually triaging takes 2 days. Fine-tuning a cheap model to do it in milliseconds.

**Process:**
1. Export 3 months of historical tickets (5,000+ examples) with human-assigned categories
2. Clean: remove PII, normalize category names, deduplicate
3. Split 80/20 train/validation
4. Fine-tune `gpt-4o-mini` (cheap base, fast inference)
5. Evaluate: compare accuracy to base model with system prompt
6. If accuracy ≥ 95% on held-out validation, promote to production

**Expected results:**
- Base `gpt-4o-mini` with system prompt: 78% accuracy
- Fine-tuned `gpt-4o-mini`: 94% accuracy
- Inference cost: 80% cheaper than using GPT-4o with few-shot examples
- Latency: 200ms vs 2-3s for few-shot prompting

### Scenario 2: Format Adaptation for Code Generation

A company uses a specific TypeScript coding style (Result types, specific error patterns). Few-shot prompting requires 2,000 tokens of examples on every request. Fine-tuning amortizes this cost.

**Dataset construction:**
- 200 pairs of: specification → TypeScript code following company standards
- Generated by GPT-4 from internal code + reviewed by senior engineers
- Focus on: Result types, error handling, naming conventions, folder structure

**Expected results:**
- Removes 2,000 tokens of few-shot examples from every request (60% cost reduction)
- Model consistently applies conventions without reminder
- Still use RAG for codebase-specific context (not fine-tuning)

## Further Reading

- [OpenAI Fine-Tuning Guide](https://platform.openai.com/docs/guides/fine-tuning)
- [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft)
- [LoRA Paper (Hu et al., 2021)](https://arxiv.org/abs/2106.09685)
- [QLoRA Paper (Dettmers et al., 2023)](https://arxiv.org/abs/2305.14314)
- [Axolotl — Fine-Tuning Framework](https://github.com/OpenAccess-AI-Collective/axolotl) — simplified LoRA/QLoRA training
- [Anthropic Claude Fine-Tuning](https://docs.anthropic.com/en/docs/about-claude/models) — enterprise plans

## Summary

Fine-tuning adapts a pre-trained LLM to a specific task by training on your data. Choose it when prompting and RAG aren't sufficient, when you need cost/latency reduction at scale, or when output format consistency is critical.

Key approaches:
- **Full fine-tuning** — all weights updated; expensive; rarely needed
- **LoRA** — trains small adapter matrices; 100x fewer parameters; same quality
- **QLoRA** — LoRA + 4-bit quantization; enables 7B models on single GPU
- **OpenAI/Anthropic API fine-tuning** — managed, no GPU needed, best starting point

Data is everything:
- 500 high-quality examples > 5,000 noisy ones
- Always hold out 20% for validation
- Use the exact same system prompt in training and inference
- Review generated data before using it as training data

Evaluation process:
- Establish base model baseline before fine-tuning
- Measure on a held-out test set (not the validation set used during training)
- For classification: accuracy, F1, confusion matrix
- For generation: LLM-as-judge or human evaluation

Decision rule: try prompting first, then RAG, then fine-tuning. Fine-tuning is the tool of last resort — powerful but expensive to maintain.
